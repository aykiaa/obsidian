{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import random\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.utils import to_networkx # Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear                   # Define layers\n",
    "from torch_geometric.nn import GCNConv\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d \n",
    "import pywt # pip install PyWavelets\n",
    "from scipy.signal import medfilt\n",
    "import cv2 # pip install opencv-python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGAR OS DADOS\n",
    "\n",
    "def carregar_ecgs(unlabel, umdavb, rbbb, lbbb, sb, st, af, filtrado):\n",
    "\n",
    "    caminho_arquivo = \"../../Projeto/Database/exams.csv\"\n",
    "    dados = pd.read_csv(caminho_arquivo)\n",
    "    arquivos_usados = [\"exams_part0.hdf5\", \"exams_part1.hdf5\",\n",
    "                    \"exams_part2.hdf5\", \"exams_part3.hdf5\", \"exams_par4.hdf5\", \"exams_part5.hdf5\",\n",
    "                    \"exams_part6.hdf5\", \"exams_part7.hdf5\", \"exams_par8.hdf5\", \"exams_part9.hdf5\",\n",
    "                    \"exams_part10.hdf5\", \"exams_part11.hdf5\", \"exams_part12.hdf5\", \"exams_part13.hdf5\", \n",
    "                    \"exams_part14.hdf5\", \"exams_part15.hdf5\", \"exams_part16.hdf5\", \"exams_part17.hdf5\"]\n",
    "\n",
    "    ecg_normal_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) ]\n",
    "    \n",
    "    ecg_umdavb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == True) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_rbbb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == True) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_lbbb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == True) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_sb_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == True) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_st_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == True) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_af_linhas = dados.index[(dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == True) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "\n",
    "    caminho_interferencias = \"../../Projeto/Database/resultados_interferencia.csv\"\n",
    "    interferencias = pd.read_csv(caminho_interferencias)\n",
    "    interferencias_ids = interferencias['exam_id'].tolist()\n",
    "\n",
    "    ecg_normal_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) ]\n",
    "    \n",
    "    ecg_umdavb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == True) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_rbbb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == True) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_lbbb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == True) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_sb_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == True) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_st_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == True) &\n",
    "                                    (dados.iloc[:, 9] == False) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "    \n",
    "    ecg_af_linhas = dados.index[~dados['exam_id'].isin(interferencias_ids) &\n",
    "                                    (dados.iloc[:, 14].isin(arquivos_usados)) &\n",
    "                                    (dados.iloc[:, 4] == False) &\n",
    "                                    (dados.iloc[:, 5] == False) &\n",
    "                                    (dados.iloc[:, 6] == False) &\n",
    "                                    (dados.iloc[:, 7] == False) &\n",
    "                                    (dados.iloc[:, 8] == False) &\n",
    "                                    (dados.iloc[:, 9] == True) &\n",
    "                                    (dados.iloc[:, 13] == False)]\n",
    "\n",
    "    print(\"Tirando Interferência:\")\n",
    "    print(\"Número de linhas ecg_normal_linhas:\", len(ecg_normal_linhas))\n",
    "    print(\"Número de linhas ecg_umdavb_linhas:\", len(ecg_umdavb_linhas))\n",
    "    print(\"Número de linhas ecg_rbbb_linhas:\", len(ecg_rbbb_linhas))\n",
    "    print(\"Número de linhas ecg_lbbb_linhas:\", len(ecg_lbbb_linhas))\n",
    "    print(\"Número de linhas ecg_sb_linhas:\", len(ecg_sb_linhas))\n",
    "    print(\"Número de linhas ecg_st_linhas:\", len(ecg_st_linhas))\n",
    "    print(\"Número de linhas ecg_af_linhas:\", len(ecg_af_linhas))\n",
    "\n",
    "    ecg_normal_id = dados.iloc[ecg_normal_linhas, 0].tolist()\n",
    "    ecg_umdavb_id = dados.iloc[ecg_umdavb_linhas, 0].tolist()\n",
    "    ecg_rbbb_id = dados.iloc[ecg_rbbb_linhas, 0].tolist()\n",
    "    ecg_lbbb_id = dados.iloc[ecg_lbbb_linhas, 0].tolist()\n",
    "    ecg_sb_id = dados.iloc[ecg_sb_linhas, 0].tolist()\n",
    "    ecg_st_id = dados.iloc[ecg_st_linhas, 0].tolist()\n",
    "    ecg_af_id = dados.iloc[ecg_af_linhas, 0].tolist()\n",
    "\n",
    "    random.seed(42) \n",
    "\n",
    "    ecg_normal_sample = random.sample(ecg_normal_id, unlabel) if len(ecg_normal_id) >= unlabel else ecg_normal_id\n",
    "    ecg_umdavb_sample = random.sample(ecg_umdavb_id, umdavb) if len(ecg_umdavb_id) >= umdavb else ecg_umdavb_id\n",
    "    ecg_rbbb_sample = random.sample(ecg_rbbb_id, rbbb) if len(ecg_rbbb_id) >= rbbb else ecg_rbbb_id\n",
    "    ecg_lbbb_sample = random.sample(ecg_lbbb_id, lbbb) if len(ecg_lbbb_id) >= lbbb else ecg_lbbb_id\n",
    "    ecg_sb_sample = random.sample(ecg_sb_id, sb) if len(ecg_sb_id) >= sb else ecg_sb_id\n",
    "    ecg_st_sample = random.sample(ecg_st_id, st) if len(ecg_st_id) >= st else ecg_st_id\n",
    "    ecg_af_sample = random.sample(ecg_af_id, af) if len(ecg_af_id) >= af else ecg_af_id\n",
    "\n",
    "    ids_ecgs = ecg_normal_sample + ecg_umdavb_sample + ecg_rbbb_sample + ecg_lbbb_sample + ecg_sb_sample + ecg_st_sample + ecg_af_sample\n",
    "\n",
    "    print(\"Número de ecgs pra usar:\", len(ids_ecgs))\n",
    "\n",
    "    \n",
    "    if filtrado == True: arquivos_hdf5 = [\"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_0_1.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_2_3.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_4_5.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_6_7.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_8_9.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_10_11.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_12_13.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_14_15.hdf5\",\n",
    "                        \"/scratch/guilherme.evangelista/Clustering-Paper/Projeto/Database/filtered_exams_16_17.hdf5\"]\n",
    "    \n",
    "    else: arquivos_hdf5 = ['/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part0.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part1.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part2.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part3.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part4.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part5.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part6.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part7.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part8.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part9.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part10.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part11.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part12.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part13.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part14.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part15.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part16.hdf5',\n",
    "                 '/scratch/pedro.bacelar/Clustering-Paper/Projeto/Database/exams_part17.hdf5']\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_ecg_data(file_path, exam_id):\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            # Obter os IDs dos exames\n",
    "            exam_ids = np.array(f['exam_id'])\n",
    "\n",
    "            # Encontrar o índice correspondente ao exam_id de interesse\n",
    "            exam_index = np.where(exam_ids == exam_id)[0]\n",
    "\n",
    "            if len(exam_index) == 0:\n",
    "                raise ValueError(\"Exam ID não encontrado.\")\n",
    "            else:\n",
    "                exam_index = exam_index[0]\n",
    "                # Acessar os tracings de ECG correspondentes ao exam_index\n",
    "                exam_tracings = f['tracings'][exam_index]\n",
    "                # Preencher tracings nulos com epsilon\n",
    "                return exam_tracings\n",
    "\n",
    "    exam_ids_to_cluster = ids_ecgs  # Substitua pelos IDs reais dos exames\n",
    "\n",
    "    # Lista para armazenar todos os tracings de ECG\n",
    "    all_tracings = []\n",
    "\n",
    "    # Obter os tracings de ECG para cada exam_id e armazenar na lista\n",
    "    for exam_id in exam_ids_to_cluster:\n",
    "        found = False  # Sinalizador para verificar se o exame foi encontrado em algum arquivo\n",
    "        for arquivo in arquivos_hdf5:\n",
    "            try:\n",
    "                tracings = get_ecg_data(arquivo, exam_id)\n",
    "                if tracings is not None:\n",
    "                    tracing_transposto = np.array(tracings).T\n",
    "                    all_tracings.append(tracing_transposto)\n",
    "                    found = True  # Sinalizador para indicar que o exame foi encontrado\n",
    "                    break  # Se encontrou, não precisa continuar buscando nos outros arquivos\n",
    "            except ValueError as e:\n",
    "                i = 0\n",
    "            except Exception as e:\n",
    "                i = 0\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"Erro: exame ID {exam_id} não encontrado em nenhum dos arquivos.\")\n",
    "\n",
    "    # Verifique o tamanho da lista all_tracings para garantir que os dados foram coletados corretamente\n",
    "    print(\"Número de ecgs que eram pra ser processados:\", len(ids_ecgs))\n",
    "    print(f\"Número total de traçados processados: {len(all_tracings)}\")\n",
    "\n",
    "    # X será um array com um único array dentro, contendo todos os números do tracings.T\n",
    "    X = np.array(all_tracings)\n",
    "    return X , ids_ecgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas ecg_normal_linhas: 272407\n",
      "Número de linhas ecg_umdavb_linhas: 3735\n",
      "Número de linhas ecg_rbbb_linhas: 6808\n",
      "Número de linhas ecg_lbbb_linhas: 4176\n",
      "Número de linhas ecg_sb_linhas: 4300\n",
      "Número de linhas ecg_st_linhas: 6146\n",
      "Número de linhas ecg_af_linhas: 4964\n",
      "Tirando Interferência:\n",
      "Número de linhas ecg_normal_linhas: 252167\n",
      "Número de linhas ecg_umdavb_linhas: 3651\n",
      "Número de linhas ecg_rbbb_linhas: 6703\n",
      "Número de linhas ecg_lbbb_linhas: 4122\n",
      "Número de linhas ecg_sb_linhas: 4248\n",
      "Número de linhas ecg_st_linhas: 6038\n",
      "Número de linhas ecg_af_linhas: 4804\n",
      "Número de ecgs pra usar: 6000\n",
      "Número de ecgs que eram pra ser processados: 6000\n",
      "Número total de traçados processados: 6000\n"
     ]
    }
   ],
   "source": [
    "X, ids_ecgs = carregar_ecgs(unlabel=50,umdavb=50,rbbb=50,lbbb=50,sb=50,st=50,af=50,filtrado=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos dados carregados: (6000, 12, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensão dos dados carregados:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_my_dataset(X, unlabel=50,umdavb=50,rbbb=50,lbbb=50,sb=50,st=50,af=50,train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Prepara o dataset de ECG para o treinamento.\n",
    "    \n",
    "    Parâmetros:\n",
    "      - X: array de ECG (formato: [n_samples, n_channels, length])\n",
    "      - os números de amostras por classe (deve coincidir com a ordem de concatenação na função carregar_ecgs)\n",
    "      - train_ratio: proporção dos dados para treinamento\n",
    "      \n",
    "    Retorna:\n",
    "      - X (possivelmente normalizado),\n",
    "      - y: vetor de labels (0: normal, 1: umdavb, 2: rbbb, 3: lbbb, 4: sb, 5: st, 6: af)\n",
    "      - train_idx: índices de treinamento\n",
    "      - test_idx: índices de teste\n",
    "    \"\"\"\n",
    "    \n",
    "    total_samples = unlabel + umdavb + rbbb + lbbb + sb + st + af\n",
    "    if X.shape[0] != total_samples:\n",
    "        raise ValueError(f\"O número de traçados em X ({X.shape[0]}) não corresponde à soma esperada ({total_samples}).\")\n",
    "    \n",
    "    # Cria os labels de acordo com a ordem de concatenação\n",
    "    y = np.array([0]*unlabel + \n",
    "                 [1]*umdavb + \n",
    "                 [2]*rbbb + \n",
    "                 [3]*lbbb + \n",
    "                 [4]*sb + \n",
    "                 [5]*st + \n",
    "                 [6]*af)\n",
    "    \n",
    "    # Aqui assumimos que a normalização é feita sobre o último eixo (tempo)\n",
    "    X_norm = X.copy().astype(np.float32)\n",
    "    for i in range(X_norm.shape[0]):\n",
    "        # Evita divisão por zero\n",
    "        mean_val = X_norm[i].mean()\n",
    "        std_val = X_norm[i].std() if X_norm[i].std() != 0 else 1.0\n",
    "        X_norm[i] = (X_norm[i] - mean_val) / std_val\n",
    "    \n",
    "    # Cria a divisão em treinamento e teste (shuffle dos índices)\n",
    "    indices = np.arange(total_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(train_ratio * total_samples)\n",
    "    train_idx = indices[:split]\n",
    "    test_idx = indices[split:]\n",
    "    \n",
    "    return X_norm, y, train_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "class SimTSCTrainer:\n",
    "    def __init__(self, device, logger):\n",
    "        self.device = device\n",
    "        self.logger = logger\n",
    "        self.tmp_dir = 'tmp'\n",
    "        if not os.path.exists(self.tmp_dir):\n",
    "            os.makedirs(self.tmp_dir)\n",
    "\n",
    "    def fit(self, model, X, y, train_idx, distances, K, alpha, test_idx=None, report_test=False, batch_size=128, epochs=500):\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "\n",
    "        train_batch_size = min(batch_size//2, len(train_idx))\n",
    "        other_idx = np.array([i for i in range(len(X)) if i not in train_idx])\n",
    "        other_batch_size = min(batch_size - train_batch_size, len(other_idx))\n",
    "        train_dataset = Dataset(train_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        if report_test and test_idx is not None:\n",
    "            test_batch_size = min(batch_size//2, len(test_idx))\n",
    "            other_idx_test = np.array([i for i in range(len(X)) if i not in test_idx])\n",
    "            other_batch_size_test = min(batch_size - test_batch_size, len(other_idx_test))\n",
    "            test_dataset = Dataset(test_idx)\n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        self.adj = torch.from_numpy(distances.astype(np.float32))\n",
    "        self.X, self.y = torch.from_numpy(X), torch.from_numpy(y)\n",
    "        file_path = os.path.join(self.tmp_dir, str(uuid.uuid4()))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=4e-3)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            for sampled_train_idx in train_loader:\n",
    "                sampled_other_idx = np.random.choice(other_idx, other_batch_size, replace=False)\n",
    "                idx = np.concatenate((sampled_train_idx, sampled_other_idx))\n",
    "                _X = self.X[idx].to(self.device)\n",
    "                _y = self.y[sampled_train_idx].to(self.device)\n",
    "                _adj = self.adj[idx][:, idx]\n",
    "                outputs = model(_X, _adj, K, alpha)\n",
    "                loss = F.nll_loss(outputs[:len(sampled_train_idx)], _y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            acc = compute_accuracy(\n",
    "                model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "                train_loader, self.device, other_idx, other_batch_size\n",
    "            )\n",
    "            \n",
    "            # --------------------------------------------------------------------------\n",
    "            # ADIÇÃO DO F1 SCORE: cálculo do F1 para o conjunto de treinamento\n",
    "            # --------------------------------------------------------------------------\n",
    "            f1 = compute_f1(\n",
    "                model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "                train_loader, self.device, other_idx, other_batch_size\n",
    "            )\n",
    "\n",
    "            if acc >= best_acc:\n",
    "                best_acc = acc\n",
    "                torch.save(model.state_dict(), file_path)\n",
    "\n",
    "            if report_test and test_idx is not None:\n",
    "                test_acc = compute_accuracy(\n",
    "                    model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "                    test_loader, self.device, other_idx_test, other_batch_size_test\n",
    "                )\n",
    "                \n",
    "                # ----------------------------------------------------------------------\n",
    "                # ADIÇÃO DO F1 SCORE: cálculo do F1 para o conjunto de teste\n",
    "                # ----------------------------------------------------------------------\n",
    "                test_f1 = compute_f1(\n",
    "                    model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "                    test_loader, self.device, other_idx_test, other_batch_size_test\n",
    "                )\n",
    "\n",
    "                self.logger.log('--> Epoch {}: loss {:5.4f}; accuracy: {:5.4f}; best accuracy: {:5.4f}; test accuracy: {:5.4f}'.format(\n",
    "                    epoch, loss.item(), acc, best_acc, test_acc\n",
    "                ))\n",
    "                # Log separado do F1 Score (não alteramos a linha existente; apenas adicionamos)\n",
    "                self.logger.log('----> F1: {:5.4f}; Test F1: {:5.4f}'.format(f1, test_f1))\n",
    "\n",
    "            else:\n",
    "                self.logger.log('--> Epoch {}: loss {:5.4f}; accuracy: {:5.4f}; best accuracy: {:5.4f}'.format(\n",
    "                    epoch, loss.item(), acc, best_acc\n",
    "                ))\n",
    "                # Log separado do F1 Score\n",
    "                self.logger.log('----> F1: {:5.4f}'.format(f1))\n",
    "\n",
    "        model.load_state_dict(torch.load(file_path))\n",
    "        model.eval()\n",
    "        os.remove(file_path)\n",
    "        return model\n",
    "\n",
    "    def test(self, model, test_idx, batch_size=128):\n",
    "        test_batch_size = min(batch_size//2, len(test_idx))\n",
    "        other_idx_test = np.array([i for i in range(len(self.X)) if i not in test_idx])\n",
    "        other_batch_size_test = min(batch_size - test_batch_size, len(other_idx_test))\n",
    "        test_dataset = Dataset(test_idx)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=1)\n",
    "        acc = compute_accuracy(\n",
    "            model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "            test_loader, self.device, other_idx_test, other_batch_size_test\n",
    "        )\n",
    "        \n",
    "        # --------------------------------------------------------------------------\n",
    "        # ADIÇÃO DO F1 SCORE: cálculo do F1 no teste (apenas informado aqui)\n",
    "        # --------------------------------------------------------------------------\n",
    "        f1 = compute_f1(\n",
    "            model, self.X, self.y, self.adj, self.K, self.alpha,\n",
    "            test_loader, self.device, other_idx_test, other_batch_size_test\n",
    "        )\n",
    "\n",
    "        # Se quiser logar aqui, poderia adicionar algo como:\n",
    "        # self.logger.log(f'Test F1: {f1:.4f}')\n",
    "        \n",
    "        return acc.item()\n",
    "\n",
    "def compute_accuracy(model, X, y, adj, K, alpha, loader, device, other_idx, other_batch_size):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in loader:\n",
    "            sampled_other_idx = np.random.choice(other_idx, other_batch_size, replace=False)\n",
    "            idx = np.concatenate((batch_idx, sampled_other_idx))\n",
    "            _X = X[idx].to(device)\n",
    "            _y = y[idx][:len(batch_idx)].to(device)\n",
    "            _adj = adj[idx][:, idx]\n",
    "            outputs = model(_X, _adj, K, alpha)\n",
    "            preds = outputs[:len(batch_idx)].max(1)[1].type_as(_y)\n",
    "            _correct = preds.eq(_y).double()\n",
    "            correct += _correct.sum()\n",
    "            total += len(batch_idx)\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ADIÇÃO DO F1 SCORE: função específica para computar o F1\n",
    "# ------------------------------------------------------------------------------\n",
    "def compute_f1(model, X, y, adj, K, alpha, loader, device, other_idx, other_batch_size):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in loader:\n",
    "            sampled_other_idx = np.random.choice(other_idx, other_batch_size, replace=False)\n",
    "            idx = np.concatenate((batch_idx, sampled_other_idx))\n",
    "            _X = X[idx].to(device)\n",
    "            _y = y[idx][:len(batch_idx)].to(device)\n",
    "            _adj = adj[idx][:, idx]\n",
    "            outputs = model(_X, _adj, K, alpha)\n",
    "            preds = outputs[:len(batch_idx)].max(1)[1]\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(_y.cpu().numpy().tolist())\n",
    "    # Usamos 'macro' por ser comum em multi-classes (pode ajustar se preferir)\n",
    "    return f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "class SimTSC(nn.Module):\n",
    "    def __init__(self, input_size, nb_classes, num_layers=1, n_feature_maps=64, dropout=0.5):\n",
    "        super(SimTSC, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.block_1 = ResNetBlock(input_size, n_feature_maps)\n",
    "        self.block_2 = ResNetBlock(n_feature_maps, n_feature_maps)\n",
    "        self.block_3 = ResNetBlock(n_feature_maps, n_feature_maps)\n",
    "        if self.num_layers == 1:\n",
    "            self.gc1 = GraphConvolution(n_feature_maps, nb_classes)\n",
    "        elif self.num_layers == 2:\n",
    "            self.gc1 = GraphConvolution(n_feature_maps, n_feature_maps)\n",
    "            self.gc2 = GraphConvolution(n_feature_maps, nb_classes)\n",
    "            self.dropout = dropout\n",
    "        elif self.num_layers == 3:\n",
    "            self.gc1 = GraphConvolution(n_feature_maps, n_feature_maps)\n",
    "            self.gc2 = GraphConvolution(n_feature_maps, n_feature_maps)\n",
    "            self.gc3 = GraphConvolution(n_feature_maps, nb_classes)\n",
    "            self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj, K, alpha):\n",
    "        ranks = torch.argsort(adj, dim=1)\n",
    "        sparse_index = [[], []]\n",
    "        sparse_value = []\n",
    "        for i in range(len(adj)):\n",
    "            _sparse_value = []\n",
    "            for j in ranks[i][:K]:\n",
    "                sparse_index[0].append(i)\n",
    "                sparse_index[1].append(j)\n",
    "                _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
    "            _sparse_value = np.array(_sparse_value)\n",
    "            _sparse_value /= _sparse_value.sum()\n",
    "            sparse_value.extend(_sparse_value.tolist())\n",
    "        sparse_index = torch.LongTensor(sparse_index)\n",
    "        sparse_value = torch.FloatTensor(sparse_value)\n",
    "        adj = torch.sparse.FloatTensor(sparse_index, sparse_value, adj.size())\n",
    "        device = self.gc1.bias.device\n",
    "        adj = adj.to(device)\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1]).squeeze()\n",
    "\n",
    "        if self.num_layers == 1:\n",
    "            x = self.gc1(x, adj)\n",
    "        elif self.num_layers == 2:\n",
    "            x = F.relu(self.gc1(x, adj))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = self.gc2(x, adj)\n",
    "        elif self.num_layers == 3:\n",
    "            x = F.relu(self.gc1(x, adj))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = F.relu(self.gc2(x, adj))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = self.gc3(x, adj)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(0))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.expand = True if in_channels < out_channels else False\n",
    "        self.conv_x = nn.Conv1d(in_channels, out_channels, 7, padding=3)\n",
    "        self.bn_x = nn.BatchNorm1d(out_channels)\n",
    "        self.conv_y = nn.Conv1d(out_channels, out_channels, 5, padding=2)\n",
    "        self.bn_y = nn.BatchNorm1d(out_channels)\n",
    "        self.conv_z = nn.Conv1d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn_z = nn.BatchNorm1d(out_channels)\n",
    "        if self.expand:\n",
    "            self.shortcut_y = nn.Conv1d(in_channels, out_channels, 1)\n",
    "        self.bn_shortcut_y = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn_x(self.conv_x(x)))\n",
    "        out = F.relu(self.bn_y(self.conv_y(out)))\n",
    "        out = self.bn_z(self.conv_z(out))\n",
    "        if self.expand:\n",
    "            x = self.shortcut_y(x)\n",
    "        x = self.bn_shortcut_y(x)\n",
    "        out += x\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, idx):\n",
    "        self.idx = idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.idx[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Dispositivo de treinamento: cuda:0\n",
      "Calculando matriz DTW com paralelismo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando DTW: 100%|██████████| 6000/6000 [42:23<00:00,  2.36linha/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cálculo DTW finalizado em 42.40 minutos.\n",
      "Iniciando o treinamento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:209: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
      "  adj = torch.sparse.FloatTensor(sparse_index, sparse_value, adj.size())\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 0: loss 1.5109; accuracy: 0.5871; best accuracy: 0.5871; test accuracy: 0.5908\n",
      "----> F1: 0.2913; Test F1: 0.2980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 1: loss 1.1036; accuracy: 0.6096; best accuracy: 0.6096; test accuracy: 0.6142\n",
      "----> F1: 0.3163; Test F1: 0.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 2: loss 1.2637; accuracy: 0.6150; best accuracy: 0.6150; test accuracy: 0.6158\n",
      "----> F1: 0.3284; Test F1: 0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 3: loss 1.1449; accuracy: 0.6292; best accuracy: 0.6292; test accuracy: 0.6167\n",
      "----> F1: 0.3684; Test F1: 0.3487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 4: loss 1.1172; accuracy: 0.6215; best accuracy: 0.6292; test accuracy: 0.6250\n",
      "----> F1: 0.3615; Test F1: 0.3682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 5: loss 1.0758; accuracy: 0.6379; best accuracy: 0.6379; test accuracy: 0.6250\n",
      "----> F1: 0.3846; Test F1: 0.3635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 6: loss 0.9117; accuracy: 0.6352; best accuracy: 0.6379; test accuracy: 0.6250\n",
      "----> F1: 0.4117; Test F1: 0.3997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 7: loss 0.9159; accuracy: 0.6444; best accuracy: 0.6444; test accuracy: 0.6342\n",
      "----> F1: 0.3942; Test F1: 0.3954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 8: loss 1.0112; accuracy: 0.6665; best accuracy: 0.6665; test accuracy: 0.6467\n",
      "----> F1: 0.4791; Test F1: 0.4645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 9: loss 1.2217; accuracy: 0.6560; best accuracy: 0.6665; test accuracy: 0.6333\n",
      "----> F1: 0.4377; Test F1: 0.4106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 10: loss 1.1265; accuracy: 0.6621; best accuracy: 0.6665; test accuracy: 0.6533\n",
      "----> F1: 0.4662; Test F1: 0.4625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 11: loss 0.7715; accuracy: 0.6821; best accuracy: 0.6821; test accuracy: 0.6692\n",
      "----> F1: 0.4938; Test F1: 0.4843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 12: loss 0.8884; accuracy: 0.6844; best accuracy: 0.6844; test accuracy: 0.6708\n",
      "----> F1: 0.5031; Test F1: 0.4894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n",
      "/tmp/ipykernel_3130654/2889164026.py:203: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  _sparse_value.append(1/np.exp(alpha*adj[i][j]))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from dtaidistance import dtw\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "        \n",
    "    def log(self, content):\n",
    "        print(content)\n",
    "        self.f.write(content + '\\n')\n",
    "        self.f.flush()\n",
    "\n",
    "def train_model(X, y, train_idx, test_idx, distances, device, K=3, alpha=0.3, epochs=500):\n",
    "    nb_classes = len(np.unique(y))\n",
    "    input_size = X.shape[1]\n",
    "    \n",
    "    model = SimTSC(input_size, nb_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    log_dir = './logs'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "    log_path = os.path.join(log_dir, 'simtsc_ecg_log.txt')\n",
    "    with open(log_path, 'w') as f:\n",
    "        logger = Logger(f)\n",
    "        trainer = SimTSCTrainer(device, logger)\n",
    "        \n",
    "        print(\"Iniciando o treinamento...\")\n",
    "        model = trainer.fit(\n",
    "            model, X, y, train_idx, distances, \n",
    "            K, alpha, test_idx=test_idx, \n",
    "            report_test=True, epochs=epochs\n",
    "        )\n",
    "        \n",
    "        acc = trainer.test(model, test_idx)\n",
    "        logger.log(f'--> Test Accuracy: {acc:.4f}')\n",
    "    \n",
    "    return model, acc\n",
    "\n",
    "def process_i(i, X_single, n_samples, r):\n",
    "    \"\"\"\n",
    "    Processa a linha i da matriz de distâncias DTW de forma paralela.\n",
    "    \"\"\"\n",
    "    local_row = np.ascontiguousarray(X_single[i], dtype=np.float64).copy()\n",
    "    local_row.setflags(write=True)  # Garante que é mutável\n",
    "    \n",
    "    dists = []\n",
    "    for j in range(i, n_samples):\n",
    "        target = np.ascontiguousarray(X_single[j], dtype=np.float64).copy()\n",
    "        target.setflags(write=True)  # Garante que é mutável\n",
    "        \n",
    "        dist = dtw.distance_fast(local_row, target, window=r)\n",
    "        dists.append(dist)\n",
    "    \n",
    "    return i, dists\n",
    "\n",
    "def get_dtw_for_ecg_parallel(X, r=100, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Calcula a matriz de distâncias DTW usando paralelismo para acelerar a computação.\n",
    "    \n",
    "    Parâmetros:\n",
    "        X (numpy array): Dados de entrada (shape: [n amostras, canais, comprimento da série]).\n",
    "        r (int): Janela de restrição para o cálculo DTW.\n",
    "        n_jobs (int): Número de núcleos para paralelismo (-1 usa todos disponíveis).\n",
    "    \n",
    "    Retorna:\n",
    "        distances (numpy array): Matriz de distâncias DTW simétrica.\n",
    "    \"\"\"\n",
    "    # Garante que os dados estão no formato correto e mutáveis\n",
    "    X_single = np.asarray(X[:, 0, :], dtype=np.float64).copy()\n",
    "    X_single.setflags(write=True)\n",
    "    \n",
    "    n_samples = X_single.shape[0]\n",
    "    distances = np.zeros((n_samples, n_samples), dtype=np.float64)\n",
    "\n",
    "    print(\"Calculando matriz DTW com paralelismo...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Processamento paralelo com barra de progresso manual\n",
    "    with tqdm(total=n_samples, desc=\"Calculando DTW\", unit=\"linha\") as pbar:\n",
    "        for res in Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "            delayed(process_i)(i, X_single, n_samples, r) for i in range(n_samples)\n",
    "        ):\n",
    "            results.append(res)\n",
    "            pbar.update(1)  # Atualiza a barra de progresso manualmente\n",
    "\n",
    "    # Preenchimento da matriz de distâncias\n",
    "    for i, dists in results:\n",
    "        j_indices = np.arange(i, n_samples)\n",
    "        distances[i, j_indices] = dists\n",
    "        distances[j_indices, i] = dists  # Garante simetria\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nCálculo DTW finalizado em {elapsed_time / 60:.2f} minutos.\")\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--> Dispositivo de treinamento: {device}\")\n",
    "\n",
    "# Pré-processamento com tipos corretos\n",
    "X_norm, y, train_idx, test_idx = get_my_dataset(X)\n",
    "X_norm = X_norm.astype(np.float64).copy()  # Mantém mutável para DTW\n",
    "\n",
    "# Cálculo DTW com paralelismo e barra de progresso\n",
    "distances = get_dtw_for_ecg_parallel(X_norm)\n",
    "\n",
    "# Treinamento (converter para float32 apenas se necessário pelo modelo)\n",
    "X_norm = X_norm.astype(np.float32)  \n",
    "\n",
    "model, test_accuracy = train_model(\n",
    "    X_norm, y, train_idx, test_idx, \n",
    "    distances, device, K=3, alpha=0.3, epochs=500\n",
    ")\n",
    "\n",
    "print(f\"Final accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\\ndef evaluate_model(model, X, y, test_idx, device):\\n    model.eval()\\n    y_true = []\\n    y_pred = []\\n    \\n    with torch.no_grad():\\n        for idx in test_idx:\\n            _X = X[idx].unsqueeze(0).to(device)\\n            _y = y[idx].item()\\n            output = model(_X)\\n            pred = output.argmax(dim=1).item()\\n            \\n            y_true.append(_y)\\n            y_pred.append(pred)\\n    \\n    accuracy = accuracy_score(y_true, y_pred)\\n    precision = precision_score(y_true, y_pred, average=\\'weighted\\', zero_division=0)\\n    recall = recall_score(y_true, y_pred, average=\\'weighted\\', zero_division=0)\\n    f1 = f1_score(y_true, y_pred, average=\\'weighted\\', zero_division=0)\\n    \\n    print(f\"Accuracy: {accuracy:.4f}\")\\n    print(f\"Precision: {precision:.4f}\")\\n    print(f\"Recall: {recall:.4f}\")\\n    print(f\"F1-score: {f1:.4f}\")\\n\\n# Chamar a função de avaliação após o treinamento do modelo\\nevaluate_model(model, X_norm, y, test_idx, device)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, X, y, test_idx, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in test_idx:\n",
    "            _X = X[idx].unsqueeze(0).to(device)\n",
    "            _y = y[idx].item()\n",
    "            output = model(_X)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "            \n",
    "            y_true.append(_y)\n",
    "            y_pred.append(pred)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Chamar a função de avaliação após o treinamento do modelo\n",
    "evaluate_model(model, X_norm, y, test_idx, device)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
